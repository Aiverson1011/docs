<Type Name="RecognizedAudio" FullName="System.Speech.Recognition.RecognizedAudio">
  <TypeSignature Language="C#" Value="public class RecognizedAudio" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi serializable beforefieldinit RecognizedAudio extends System.Object" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.RecognizedAudio" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary>関連付けられている表しますオーディオの入力は、 <see cref="T:System.Speech.Recognition.RecognitionResult" />です。</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## 解説  
 音声認識エンジンでは、認識操作の一部として、オーディオ入力に関する情報を生成します。 認識されたオーディオにアクセスするには、使用、 <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> プロパティまたは <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> のメソッド、 <xref:System.Speech.Recognition.RecognitionResult>です。  
  
 次のイベントとのメソッドによって認識結果を生成することができます、 <xref:System.Speech.Recognition.SpeechRecognizer> と <xref:System.Speech.Recognition.SpeechRecognitionEngine> クラス。  
  
-   イベント:  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>  
  
-   方法:  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> および <xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>  
  
    -   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>  
  
> [!IMPORTANT]
>  エミュレートされた音声認識によって生成される認識結果に認識されているオーディオが含まれていません。 このような認識結果では、その <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> プロパティを返します。 `null` とその <xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> メソッドは例外をスローします。 エミュレートされた音声認識の詳細については、次を参照してください。、 `EmulateRecognize` と `EmulateRecognizeAsync` のメソッド、 <xref:System.Speech.Recognition.SpeechRecognizer> と <xref:System.Speech.Recognition.SpeechRecognitionEngine> クラスです。  
  
   
  
## 例  
 次の例のハンドル、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, 、<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, 、または <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> イベントと認識された音声認識結果に関連付けられている情報をコンソールに出力します。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
    <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>開始時に認識されているオーディオの入力オーディオ ストリーム内の位置を取得します。</summary>
        <value>認識されたオーディオの開始時に入力オーディオ ストリーム内の位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
 このプロパティは、認識された語句の入力デバイスの生成されたオーディオ ストリーム内の開始位置を参照します。 これに対し、 `RecognizerAudioPosition` のプロパティ、 <xref:System.Speech.Recognition.SpeechRecognitionEngine> と <xref:System.Speech.Recognition.SpeechRecognizer> クラスは、オーディオ入力内の認識エンジンの位置を参照します。 それらの位置は異なってもかまいません。 詳細については、「[Using Speech Recognition Events](http://msdn.microsoft.com/ja-jp/01c598ca-2e0e-4e89-b303-cd1cef9e8482)」を参照してください。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> プロパティが認識操作の開始時にシステム時刻を取得します。  
  
   
  
## 例  
 次の例のハンドル、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> または <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> イベントと認識された音声認識結果に関連付けられている情報をコンソールに出力します。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.Enabled" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.Duration" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Duration">
      <MemberSignature Language="C#" Value="public TimeSpan Duration { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan Duration" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Duration" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>オーディオの認識された入力オーディオ ストリームの期間を取得します。</summary>
        <value>入力オーディオ ストリーム内で認識されているオーディオの期間です。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
   
  
## 例  
 次の例のハンドル、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> または <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> イベントと認識された音声認識結果に関連付けられている情報をコンソールに出力します。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      </Docs>
    </Member>
    <Member MemberName="Format">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo Format" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.Format" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>認識エンジンによって処理されたオーディオの形式を取得します。</summary>
        <value>音声認識エンジンによって処理されたオーディオの形式です。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
   
  
## 例  
 次の例のハンドル、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> または <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> イベントと認識された音声認識結果に関連付けられている情報をコンソールに出力します。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="N:System.Speech.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="GetRange">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognizedAudio GetRange(valuetype System.TimeSpan audioPosition, valuetype System.TimeSpan duration) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizedAudio</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioPosition" Type="System.TimeSpan" />
        <Parameter Name="duration" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="audioPosition">返されるオーディオ データの開始位置。</param>
        <param name="duration">返されるセグメントの長さ。</param>
        <summary>選択し、バイナリ データとして音声認識、現在のセクションを返します。</summary>
        <returns>定義された、認識されているオーディオのサブセクションを返します <paramref name="audioPosition" /> と <paramref name="duration" />です。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
   
  
## 例  
 次の例では、名前の入力に対して音声認識文法のハンドラーを追加、 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> イベントと、プロセス内の音声認識エンジンの文法に読み込みます。 オーディオ ファイルへの入力の名前の部分のオーディオ情報を書き込みます。 オーディオ ファイルへの入力として使用する <xref:System.Speech.Synthesis.SpeechSynthesizer> オブジェクトを録音されたオーディオを含む語句を物語っています。  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <paramref name="audioPosition" /> <paramref name="duration" /> オーディオが現在のセグメントの範囲外のセグメントを定義します。</exception>
        <exception cref="T:System.InvalidOperationException">現在では、オーディオが含まれていないデータには認識します。</exception>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="StartTime">
      <MemberSignature Language="C#" Value="public DateTime StartTime { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.DateTime StartTime" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.RecognizedAudio.StartTime" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.DateTime</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>認識操作の開始時に、システム時刻を取得します。</summary>
        <value>認識操作の開始時のシステム時刻。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
 <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A> プロパティは、待機時間とパフォーマンスの計算に使用できる認識操作の開始時に、システム時刻を取得します。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A> プロパティが生成されたオーディオ ストリームの入力デバイスの位置を取得します。  
  
   
  
## 例  
 次の例のハンドル、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> または <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> イベントと認識された音声認識結果に関連付けられている情報をコンソールに出力します。  
  
```csharp  
  
// Handle the SpeechRecognized event.   
void SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  
{  
  if (e.Result == null) return;  
  
  RecognitionResult result = e.Result;  
  
  Console.WriteLine("Grammar({0}): {1}",  
    result.Grammar.Name, result.Text);  
  
  if (e.Result.Audio != null)  
  {  
    RecognizedAudio audio = e.Result.Audio;  
  
    Console.WriteLine("   start time: {0}", audio.StartTime);  
    Console.WriteLine("   encoding format: {0}", audio.Format.EncodingFormat);  
    Console.WriteLine("   position: {0}, duration: {1}",  
      audio.AudioPosition, audio.Duration);  
  }  
  
  // Add event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.RecognizedAudio.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="WriteToAudioStream">
      <MemberSignature Language="C#" Value="public void WriteToAudioStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToAudioStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">オーディオ データを受信しているストリーム。</param>
        <summary>生データとして、全体のオーディオをストリームに書き込みます。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
 オーディオ データが書き込まれる `outputStream` バイナリ形式にします。 ヘッダー情報が含まれるではありません。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> メソッドは、Wave 形式を使用しますが、Wave ヘッダーは含まれません。 Wave ヘッダーを含めるには使用、 <xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A> メソッドです。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="WriteToWaveStream">
      <MemberSignature Language="C#" Value="public void WriteToWaveStream (System.IO.Stream outputStream);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void WriteToWaveStream(class System.IO.Stream outputStream) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="outputStream" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="outputStream">オーディオ データを受信しているストリーム。</param>
        <summary>オーディオを Wave 形式のストリームに書き込みます。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## 解説  
 オーディオ データが書き込まれる `outputStream` を Wave 形式でリソース インターチェンジ ファイルの式 \(RIFF\) ヘッダーが含まれています。  
  
 <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A> メソッドは、同じバイナリ形式を使用しますが、Wave ヘッダーは含まれません。  
  
   
  
## 例  
 次の例では、名前の入力に対して音声認識文法のハンドラーを追加、 <xref:System.Speech.Recognition.Grammar.SpeechRecognized> イベントと、プロセス内の音声認識エンジンの文法に読み込みます。 オーディオ ファイルへの入力の名前の部分のオーディオ情報を書き込みます。 オーディオ ファイルへの入力として使用する <xref:System.Speech.Synthesis.SpeechSynthesizer> オブジェクトを録音されたオーディオを含む語句を物語っています。  
  
```  
private static void AddNameGrammar(SpeechRecognitionEngine recognizer)  
{  
  GrammarBuilder builder = new GrammarBuilder();  
  builder.Append("My name is");  
  builder.AppendWildcard();  
  
  Grammar nameGrammar = new Grammar(builder);  
  nameGrammar.Name = "Name Grammar";  
  nameGrammar.SpeechRecognized +=  
    new EventHandler<SpeechRecognizedEventArgs>(  
      NameSpeechRecognized);  
  
  recognizer.LoadGrammar(nameGrammar);  
}  
  
// Handle the SpeechRecognized event of the name grammar.  
private static void NameSpeechRecognized(  
  object sender, SpeechRecognizedEventArgs e)  
{  
  Console.WriteLine("Grammar ({0}) recognized speech: {1}",  
    e.Result.Grammar.Name, e.Result.Text);  
  
  try  
  {  
    // The name phrase starts after the first three words.  
    if (e.Result.Words.Count < 4)  
    {  
  
      // Add code to check for an alternate that contains the   
wildcard.  
      return;  
    }  
  
    RecognizedAudio audio = e.Result.Audio;  
    TimeSpan start = e.Result.Words[3].AudioPosition;  
    TimeSpan duration = audio.Duration - start;  
  
    // Add code to verify and persist the audio.  
    string path = @"C:\temp\nameAudio.wav";  
    using (Stream outputStream = new FileStream(path, FileMode.Create))  
    {  
      RecognizedAudio nameAudio = audio.GetRange(start, duration);  
      nameAudio.WriteToWaveStream(outputStream);  
      outputStream.Close();  
    }  
  
    Thread testThread =  
      new Thread(new ParameterizedThreadStart(TestAudio));  
    testThread.Start(path);  
  }  
  catch (Exception ex)  
  {  
    Console.WriteLine("Exception thrown while processing audio:");  
    Console.WriteLine(ex.ToString());  
  }  
}  
  
// Use the speech synthesizer to play back the .wav file  
// that was created in the SpeechRecognized event handler.  
  
private static void TestAudio(object item)  
{  
  string path = item as string;  
  if (path != null && File.Exists(path))  
  {  
    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  
    PromptBuilder builder = new PromptBuilder();  
    builder.AppendText("Hello");  
    builder.AppendAudio(path);  
    synthesizer.Speak(builder);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)" />
        <altmember cref="M:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)" />
      </Docs>
    </Member>
  </Members>
</Type>